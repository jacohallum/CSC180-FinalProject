# CSC180-FinalProject
To receive good results from our AI, we must first begin by feeding the AI proper data. While the dataset we used from Kaggle.com[2] included everything we will be used for training our AI, it also included information we will not be needing for training. Therefore, before we can proceed to train we must first prepare the data.
Following data import, we wish to clean up any extra, incomplete, or incorrect data entries. To do this, we first begin by removing any entries which are lacking data or have a null value in any of their data columns. If any entries exist then we may use the built-in drop() function to drop any specified columns. 
We may further clean up our dataset by removing any duplicate entries as well. We remove duplicates as they create a bias in the AI that leads to a higher or lower hit rate. To do so, we simply iterate over the entire dataset and check for any duplicate values. If present, we can use the same drop() function along with specifying the entries to drop. It should be noted though that dropping entries can lead to gaps in dataset indexing. Therefore we must account for this and reset the index of every entry.
As mentioned in the beginning, the dataset we received contains not only relevant information to train our AI but also irrelevant information. With incomplete and duplicate entries removed, we now must pick out what data we will be feeding to our AI. We found that irrelevant information included the origin and destination account numbers as they provided no information on if a transaction was fraudulent (fraudulent accounts can be flagged at a later time if desired) as well as the amount of money in the destination account as that value provided no useful information. With the offending columns removed, we were left with only pertinent data to train our model.
We can see that from data prepping we were left with a combination of data types.  data.types provide us with the type for each column in the dataset and we can see we were left with float 64. int 64, and objects. While float and int 64 are an accepted type for AI models, objects are not, therefore we must convert them to numeric. In this case, we will encode our object types converting them to binary classification. In essence, binary classification converts the object and every unique value into its binary column. if the value of the entire contains a 1 in the value of the specific column  then the original value of the column pre-encoding was the name of the column. With encoding complete, we are now only left with int and floats. This is precisely what we are looking for as the AI models can now read all the data but there stands one last issue before we can send the data to train. An AI model does not know what transaction amounts mean. What could an AI do with the information of a transaction that sent $10 or received $20,000. The data the AI receives must describe a relevance or a relation to another. Therefore to do this we must normalize all the original values in the dataset. Note, we must not normalize the new columns we created from encoding. Stephen Watts puts it well when he says, “Data normalization is the organization of data to appear similar across all records and fields. It increases the cohesion of entry types leading to cleansing, lead generation, segmentation, and higher-quality data. Simply put, this process includes eliminating unstructured data and redundancy (duplicates) to ensure logical data storage.”[3] Data normalization will structure the data in a way that the AI will know what to do with and with that completed we may now begin using the data to train our models
