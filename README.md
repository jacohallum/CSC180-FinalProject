# CSC180-FinalProject
This is a possible solution to an issue that has become more prevalent with the mass adoption of digital transactions. Digital transactions over the past 10 years have increased dramatically in popularity  and with this boom comes those who wish to make illegitimate transactions and to steal, and scam individuals out of their money. It is for this reason that we are proposing the use of artificial intelligence to predict and in turn cancel or stop a transaction it deems to be fraudulent. The motivation for this topic was spurred on by the idea that Banks will only occasionally stop a transaction they feel is illegitimate while all other transactions of smaller values fall on the individual to monitor. The authors of this report require the reader to reflect on the past and determine if they ever saw an unknown transaction on a bank statement and the steps that were needed to restore the purchase. Was it automated or was there a lot of work? Did you need to get a new credit card?

To receive good results from our AI, we must first begin by feeding the AI proper data. While the dataset we used from Kaggle.com[2] included everything we will be used for training our AI, it also included information we will not be needing for training. Therefore, before we can proceed to train we must first prepare the data.
Following data import, we wish to clean up any extra, incomplete, or incorrect data entries. To do this, we first begin by removing any entries which are lacking data or have a null value in any of their data columns. If any entries exist then we may use the built-in drop() function to drop any specified columns. 
We may further clean up our dataset by removing any duplicate entries as well. We remove duplicates as they create a bias in the AI that leads to a higher or lower hit rate. To do so, we simply iterate over the entire dataset and check for any duplicate values. If present, we can use the same drop() function along with specifying the entries to drop. It should be noted though that dropping entries can lead to gaps in dataset indexing. Therefore we must account for this and reset the index of every entry.
As mentioned in the beginning, the dataset we received contains not only relevant information to train our AI but also irrelevant information. With incomplete and duplicate entries removed, we now must pick out what data we will be feeding to our AI. We found that irrelevant information included the origin and destination account numbers as they provided no information on if a transaction was fraudulent (fraudulent accounts can be flagged at a later time if desired) as well as the amount of money in the destination account as that value provided no useful information. With the offending columns removed, we were left with only pertinent data to train our model.
We can see that from data prepping we were left with a combination of data types.  data.types provide us with the type for each column in the dataset and we can see we were left with float 64. int 64, and objects. While float and int 64 are an accepted type for AI models, objects are not, therefore we must convert them to numeric. In this case, we will encode our object types converting them to binary classification. In essence, binary classification converts the object and every unique value into its binary column. if the value of the entire contains a 1 in the value of the specific column  then the original value of the column pre-encoding was the name of the column. With encoding complete, we are now only left with int and floats. This is precisely what we are looking for as the AI models can now read all the data but there stands one last issue before we can send the data to train. An AI model does not know what transaction amounts mean. What could an AI do with the information of a transaction that sent $10 or received $20,000. The data the AI receives must describe a relevance or a relation to another. Therefore to do this we must normalize all the original values in the dataset. Note, we must not normalize the new columns we created from encoding. Stephen Watts puts it well when he says, “Data normalization is the organization of data to appear similar across all records and fields. It increases the cohesion of entry types leading to cleansing, lead generation, segmentation, and higher-quality data. Simply put, this process includes eliminating unstructured data and redundancy (duplicates) to ensure logical data storage.”[3] Data normalization will structure the data in a way that the AI will know what to do with and with that completed we may now begin using the data to train our models

Methodology

Dataset
For the training of our AI, we used a dataset from Kaggle.com [2]. Named “Online Payment Fraud Detection”, the dataset is a collection of over 6 million unique transactions. Each transaction contains the type of  transaction made, the origin account with start and end balance, the destination account with start and end balance, as well as the amount transferred, and if the transaction was fraudulent. The dataset provides enough information to allow for use to train an AI to determine the legitimacy of a transaction.

Splitting to train and test
For the split, we split the dataset into 20% test data and 80% training data. We found this to be optimal as it allowed for more data and a larger variety of data to be used in training. While there are over 6.5 million entries and we could have done a more even split. That would only speed up training and not introduce a bigger variety of outcomes to the model.

Experimental settings
For our experimental setting, we ran a few variations of activations and optimizers. While the main focus was to determine the best choice of activations and optimizers, we also wanted to test if adding variations to the number of convolutional layers and a number of kernels would affect the outcome. At first, we chose to run the same settings for every variation of activation and optimizer. With the baseline determined, we then ran a few variations in the number of convolutional layers and kernels to see if we could make the model more accurate.

Method to compare different Metrics 
For model comparison, we chose to calculate the scores for each model and plot the confusion matrices for each model. The scores that were calculated included the Accuracy of the model, Precision, Recall, and the F1 Score. The reason for these metrics is that we can receive a baseline on how good the models were. Accuracy will provide us with the percentage of correct predictions that were made from the test dataset. Precision will provide us with the percentage of results that were relevant to the test at hand. Recall will provide us with the percentage of total relevant results correctly classified. Lastly, the F1 score provides us with the performance of the model. The higher the F1 score, the better the model performed in testing.

Implemented methods and comparisons
In our testing, we decided to test a mixture of fully connected neural networks(FCNN) and convolutional neural networks(CNN) to find the best at predicting fraudulent online payments. Below we can see the 3 FCNNs we tested along with the 7 CNN's we tested. Following our testing, we then calculated our scores for each model and compared them against one another.

Fully Connected Neural Networks
Fully Connected Neural Network (Relu + Adam + 2 Hidden Layers, 20, 10 Neurons)
Fully Connected Neural Network (Sigmoid + Adam + 2 Hidden Layers, 20, 10 Neurons)
Fully Connected Neural Network (Tanh + Adam + 2 Hidden Layers, 20, 10 Neurons)

Convolutional Neural Networks
Convolutional Neural Network (Relu + Adam + 2 Convolution Layers with 32 and 64 kernels + Kernel Size: 1x5)
Convolutional Neural Network (Relu + SGD + 2 Convolution Layers with 32 and 64 kernels + Kernel Size: 1x5)
Convolutional Neural Network (Sigmoid + Adam + 2 Convolution Layers with 32 and 64 kernels + Kernel Size: 1x5)
Convolutional Neural Network (Sigmoid + SGD + 2 Convolution Layers with 32 and 64 kernels + Kernel Size: 1x5)
Convolutional Neural Network (Tanh + Adam + 2 Convolution Layers with 32 and 64 kernels + Kernel Size: 1x5)
Convolutional Neural Network (Tanh + SGD + 2 Convolution Layers with 32 and 64 kernels + Kernel Size: 1x5)
Convolutional Neural Network (Relu + Adam + 1 Convolution Layers with 64 kernels + Kernel Size: 1x3)

In conclusion, we were able to build an AI that was able to predict fraudulent transactions to an accuracy of 99.99% using the provided dataset from Kaggle.com[2]. While there are some optimizations we could have made in training the AI, we do feel that what we did lead to a more accurate AI.
